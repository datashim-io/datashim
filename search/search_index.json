{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#datashim","title":"Datashim","text":""},{"location":"#overview","title":"Overview","text":"<p>Datashim is a Kubernetes Framework to provide easy access to S3 and NFS Datasets within pods. It orchestrates the provisioning of Persistent Volume Claims and ConfigMaps needed for each Dataset.</p> <p>Datashim introduces the Dataset CRD which is a pointer to existing S3 and NFS data sources. It includes the necessary logic to map these Datasets into Persistent Volume Claims and ConfigMaps which users can reference in their pods, letting them focus on the workload development and not on configuring/mounting/tuning the data access. Thanks to Container Storage Interface it is extensible to support additional data sources in the future.</p> <p></p>"},{"location":"#installing","title":"Installing","text":"<p>Tip</p> <p>Make sure you also read the post-install steps as they contain important information</p>"},{"location":"#using-helm","title":"Using Helm","text":"<p>Add the Datashim Helm repository with the following commands:</p> <pre><code>helm repo add datashim https://datashim-io.github.io/datashim/\nhelm repo update\n</code></pre> <p>Once this has completed, run:</p> <pre><code>helm search repo datashim --versions\n</code></pre> <p>To verify that the repository has been added correctly. You can now install Datashim with:</p> <pre><code>helm install --namespace=dlf --create-namespace \\\n              datashim datashim/datashim-charts\n</code></pre>"},{"location":"#using-manifests","title":"Using manifests","text":"<p>If you prefer, you can install Datashim using the manifests provided. Start by creating the <code>dlf</code> namespace with:</p> <pre><code>kubectl create ns dlf\n</code></pre> <p>Then, based on your environment, execute one of the following commands:</p> <ul> <li>Kubernetes/Minikube</li> </ul> <pre><code>kubectl apply -f https://raw.githubusercontent.com/datashim-io/datashim/master/release-tools/manifests/dlf.yaml\n</code></pre> <ul> <li>Kubernetes on IBM Cloud</li> </ul> <pre><code>kubectl apply -f https://raw.githubusercontent.com/datashim-io/datashim/master/release-tools/manifests/dlf-ibm-k8s.yaml\n</code></pre> <ul> <li>Openshift</li> </ul> <pre><code>kubectl apply -f https://raw.githubusercontent.com/datashim-io/datashim/master/release-tools/manifests/dlf-oc.yaml\n</code></pre> <ul> <li>Openshift on IBM Cloud</li> </ul> <pre><code>kubectl apply -f https://raw.githubusercontent.com/datashim-io/datashim/master/release-tools/manifests/dlf-ibm-oc.yaml\n</code></pre>"},{"location":"#post-install-steps","title":"Post-install steps","text":"<p>Ensure that Datashim has been deployed correctly and ready by using the following command:</p> <pre><code>kubectl wait --for=condition=ready pods -l app.kubernetes.io/name=datashim -n dlf\n</code></pre> <p>Datashim's label-based functionalities require the <code>monitor-pods-datasets=enabled</code> annotation. To enable it in the <code>default</code> namespace, for example, you can run:</p> <pre><code>kubectl label namespace default monitor-pods-datasets=enabled\n</code></pre>"},{"location":"#using-datashim","title":"Using Datashim","text":"Warning <p>This section requires you to have an existing S3 bucket available. If you do not have one, you can deploy a local S3 server using <pre><code>kubectl apply -n dlf -f https://github.com/datashim-io/datashim/raw/master/examples/minio/minio.yaml\n</code></pre> NOTE: use this only as a reference point. For production, make sure sure appropriate and secure credentials are used</p> <p>To use Datashim, we need to create a Dataset: we can do so by editing and running the following:</p> <p>Danger</p> <p>Hardcoding your credentials in a Dataset is insecure.</p> <p>This example is provided only because of its simplicity, but we recommend storing your credentials in a Secret and referencing it in a Dataset as shown here</p> <p>Make sure you delete this example Dataset after you've tried it.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: datashim.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: example-dataset\nspec:\n  local:\n    type: \"COS\"\n    accessKeyID: \"{AWS_ACCESS_KEY_ID}\"\n    secretAccessKey: \"{AWS_SECRET_ACCESS_KEY}\"\n    endpoint: \"{S3_SERVICE_URL}\"\n    bucket: \"{BUCKET_NAME}\"\n    readonly: \"true\" #OPTIONAL, default is false\n    region: \"\" #OPTIONAL\nEOF\n</code></pre> <p>If everything worked, you should now see a PVC named <code>example-dataset</code> which you can mount in your pods. Assuming you have labeled your namespace with <code>monitor-pods-datasets=enabled</code> as mentioned in the post-install steps, you will now be able to mount the PVC in a pod as simply as this:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    dataset.0.id: \"example-dataset\"\n    dataset.0.useas: \"mount\"\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n</code></pre> <p>As a convention the Dataset will be mounted in <code>/mnt/datasets/example-dataset</code>. If instead you wish to pass the connection details as environment variables, change the <code>useas</code> line to <code>dataset.0.useas: \"configmap\"</code></p>"},{"location":"#next-steps","title":"Next Steps","text":"<p>To learn more about Datashim and how it can help you, explore our Use Cases and get started with the User Guide.</p> <ul> <li> <p> Datashim in action</p> <p>Learn more about Datashim and how it can help you in our Use Cases section</p> <p> Use Cases</p> </li> <li> <p> Get started with Datashim</p> <p>Check out our User Guide and get up and running in minutes</p> <p> User Guide</p> </li> </ul>"},{"location":"Ceph-Caching/","title":"Using Ceph for caching","text":""},{"location":"Ceph-Caching/#installation","title":"Installation","text":""},{"location":"Ceph-Caching/#method-1-recommended","title":"Method 1 (Recommended)","text":"<p>Inside <code>plugins/ceph-cache-plugin/deploy/rook</code> directory execute: <pre><code>kubectl create -f common.yaml\nkubectl create -f operator.yaml\n</code></pre> Inspect the file cluster.yaml and setup according to the nodes and the dedicated ceph-wise disk devices, the value of <code>storage.nodes</code> e.g. <pre><code>storage:\n    useAllNodes: false\n    useAllDevices: false\n    nodes:\n      - name: \"minikube\"\n        devices: \n          - name: \"sdb\"\n            config:\n              storeType: bluestore\n              osdsPerDevice: \"1\"\n</code></pre> Afterward, execute: <pre><code>kubectl create -f cluster.yaml\n</code></pre> If everything worked correctly the pods in the rook-ceph namespace should be like this: <pre><code>rook-ceph-mgr-a-5f8f5c978-xgcpw                     1/1     Running     0          79s\nrook-ceph-mon-a-6879b87656-bxbrw                    1/1     Running     0          89s\nrook-ceph-operator-86f9b59b8-2fvkb                  1/1     Running     0          5m29s\nrook-ceph-osd-0-9dcb46c48-hrzvz                     1/1     Running     0          43s\n</code></pre> NOTE If you want to delete/create a new cluster, besides invoking <code>kubectl delete -f cluster.yaml</code> You need also to delete the paths in defined in <code>dataDirHostPath</code> and directories.path</p> <p>Now we can proceed with installing DLF.</p>"},{"location":"Ceph-Caching/#method-2-testing","title":"Method 2 (Testing)","text":"<p>If you are after maximum performance we strongly advice to set up your ceph cluster according to the method above. However, for testing purposes and/or lacking of disk devices we describe a method to test this inside minikube and provide a script <code>plugins/ceph-cache-plugin/deploy/rook/setup_ceph_cluster.sh</code> that installs rook with csi-lvm storage class. </p>"},{"location":"Ceph-Caching/#minikube-installation","title":"Minikube installation","text":"<p>First we need to have a working cluster.</p> <p><code>minikube start --memory='6G' --cpus=4 --disk-size='40g' --driver=virtualbox -p rooktest</code></p> <p>NOTE: run <code>./minikube/fix_minikube_losetup.py</code> to bypass the current issue of minikube with loset.</p> <p>NOTE2: if you change the disk-size of the minikube command make sure to tune accordingly the following parameters</p>"},{"location":"Ceph-Caching/#csi-lvm-setup","title":"CSI-LVM setup","text":"<p>Before invoking the script you should tune according to your needs the following attributes</p> Attribute File Description GIGA_SPACE <code>plugins/ceph-cache-plugin/deploy/rook/csi-lvm-setup/create-loops.yaml</code> Size of the loop device that csi-lvm will create on each node <code>spec.mon.volumeClaimTemplate.spec.resources.requests.storage</code> <code>plugins/ceph-cache-plugin/deploy/rook/cluster-on-pvc.yaml</code> Storage Size of mon ceph service <code>spec.storage.storageClassDeviceSets.volumeClaimTemplates.spec.resources.requests.storage</code> <code>plugins/ceph-cache-plugin/deploy/rook/cluster-on-pvc.yaml</code> Storage size of CEPH osds <code>spec.storage.storageClassDeviceSets.count</code> <code>plugins/ceph-cache-plugin/deploy/rook/cluster-on-pvc.yaml</code> Total number of CEPH osds <p>The command line arguments of the script are the names of the nodes that the csi-lvm should create loop devices on and the corresponding CEPH services will run on, e.g.</p> <pre><code>cd plugins/ceph-cache-plugin/deploy/rook &amp;&amp; \\\n./setup_ceph_cluster.sh nodename1 ...\n</code></pre> <p>Keep in mind that the script will uninstall any previous installations of csi-lvm and rook-ceph which made through the script. If no command line arguments are passed to the script this will result in uninstalling everything.</p>"},{"location":"Ceph-Caching/#dlf-installation","title":"DLF Installation","text":"<p>Go into the root of this directory and execute: <code>make deployment</code></p> <p>The pods in the default namespace would look like this: <pre><code>csi-attacher-nfsplugin-0            2/2     Running   0          7s\ncsi-attacher-s3-0                   1/1     Running   0          8s\ncsi-nodeplugin-nfsplugin-nqgtl      2/2     Running   0          7s\ncsi-provisioner-s3-0                2/2     Running   0          8s\ncsi-s3-k9b5j                        2/2     Running   0          8s\ndataset-operator-7b8f65f7d4-hg8n5   1/1     Running   0          6s\n</code></pre> Create an s3 dataset by replacing the values and invoking <code>kubectl create -f my-dataset.yaml</code> <pre><code>apiVersion: datashim.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: example-dataset\nspec:\n  local:\n    type: \"COS\"\n    accessKeyID: \"{AWS_ACCESS_KEY_ID}\"\n    secretAccessKey: \"{AWS_SECRET_ACCESS_KEY}\"\n    endpoint: \"{S3_SERVICE_URL}\"\n    bucket: \"{BUCKET_NAME}\"\n    region: \"\" #it can be empty\n</code></pre></p> <p>Now if you check about datasetsinternal and PVC you would be able to see the example-dataset <pre><code>kubectl get datasetsinternal\nkubectl get pvc\n</code></pre> Delete the dataset we created before by executing <code>kubectl delete dataset/example-dataset</code> If you execute <code>kubectl describe datasetinternal/example-dataset</code> you would see the credentials and the endpoints you originally specified.</p> <p>Let's try to add the caching plugin.</p>"},{"location":"Ceph-Caching/#ceph-caching-plugin-installation","title":"Ceph Caching Plugin Installation","text":"<p>Change into the directory and invoke: <code>make deployment</code></p> <p>Let's create the same dataset now that the plugin is deployed: <code>kubectl create -f my-dataset.yaml</code></p> <p>You should see a new rgw pod starting up on rook-ceph namespace: <pre><code>rook-ceph-rgw-test-a-77f78b7b69-z5kp9              1/1     Running     0          4m43s\n</code></pre> After a couple of minutes if you list datasetsinternal you will see the example-dataset created.  If you describe it using <code>kubectl describe datasetinternal/example-dataset</code> you will notice that the credentials are different and they point to the rados gateway instance, therefore the PVC would reflect the cached version of the dataset.</p>"},{"location":"FAQ/","title":"FAQ","text":""},{"location":"FAQ/#what-is-the-framework-offering-exactly","title":"What is the framework offering exactly?","text":"<p>One new Custom Resource Definition: the Dataset. Essentially this CRD is a declarative way to reference an existing data source. Moreover, we provide a mount-point in user's pod for each Dataset and expose an interface for caching mechanisms to leverage. Current implementation supports S3- and NFS-based data sources.</p>"},{"location":"FAQ/#thats-it-you-just-add-one-more-crd","title":"That's it? You just add one more CRD?","text":"<p>Not quite. For every Dataset we create one Persistent Volume Claim which users can mount directly to their pods. We have implemented that logic as a regular Kubernetes Operator.</p>"},{"location":"FAQ/#what-is-the-motivation-for-this-work-what-problem-does-it-solve","title":"What is the motivation for this work? What problem does it solve?","text":"<p>Since the introduction of Container Storage Interface, there are more and more storage providers becoming available on Kubernetes environments. However we feel that for the non-experienced Kubernetes users it might be a high barrier for them to install/maintain/configure in order to leverage the available CSI plugins and gain access to the remote data sources on their pods.</p> <p>By introducing a higher level of abstraction (Dataset) and by taking care of all the necessary work around invoking the appropriate CSI plugin, configuring and provisioning the PVC we aim to improve the User Experience of data access in Kubernetes</p>"},{"location":"FAQ/#soyou-want-to-replace-csi","title":"So...you want to replace CSI?","text":"<p>On the contrary! Every type of data source we support actually comes with its own completely standalone CSI implementation.</p> <p>We are aspiring to be a meta-framework for the CSI plugins. If we have to make a comparison, we want make accessible different types of data sources  the same way Kubeflow makes Machine Learning frameworks accessible on Kubernetes</p>"},{"location":"FAQ/#are-you-competing-with-the-cosi-proposal","title":"Are you competing with the COSI proposal?","text":"<p>Absolutely no. COSI aims to manage the full lifecycle of a bucket like provisioning, configuring access etc. which is beyond our scope. We just want to offer a mountpoint for COS buckets</p>"},{"location":"FAQ/#any-other-potential-benefits-you-see-with-the-framework","title":"Any other potential benefits you see with the framework?","text":"<p>We believe that by introducing Dataset as a CRD you can accomplish higher level orchestration and bring contributions on: - Performance: We have attempted to create a pluggable caching interface like the example implementation: Ceph Caching Plugin - Security: Another effort we are exploring is to have a common access management layer for credentials of the different types of datasources </p>"},{"location":"FAQ/#is-anyone-actually-interested-in-the-framework","title":"Is anyone actually interested in the framework?","text":"<ul> <li>European Bioinformatics Institute ( https://www.ebi.ac.uk/ ) are running a POC with Datashim and Kubeflow on their cloud infrastructure</li> <li>David Yu Yuan actually reached out to us after a CNCF presentation</li> <li>People from Open Data Hub ( https://opendatahub.io/ ) are interested in integrating Datashim in ODH</li> <li>See relevant issue ( https://github.com/IBM/dataset-lifecycle-framework/issues/40 )</li> <li>Pachyderm's proposal is actually very close to the Dataset spec we are supporting.</li> <li>Datashim is forked in their repo and is under evaluation in their repo https://github.com/pachyderm/kfdata</li> </ul>"},{"location":"GitWorkflow/","title":"Git workflow for Datashim development","text":"<p>We'll roughly follow the Github development flow used by the Kubernetes project. </p> <ol> <li> <p>Visit https://github.com/datashim-io/datashim. Fork your own copy of Datashim to your Github account. For the sake of illustration, let's say this fork corresponds to <code>https://github.com/$user/datashim</code> where <code>$user</code> is your username.</p> </li> <li> <p>Go to the source directory of your Go workspace and clone your fork there. Using the example above where the workspace is in <code>$HOME/goprojects</code>,    <pre><code>$&gt; mkdir -p $HOME/goprojects/src\n$&gt; cd $HOME/goprojects/src\n$&gt; git clone https://github.com/$user/datashim.git\n</code></pre></p> </li> <li> <p>Set the Datashim repo as your upstream and rebase    <pre><code>$&gt; cd $HOME/goprojects/src/datashim\n$&gt; git remote add upstream https://github.com/datashim-io/datashim\n$&gt; git remote set-url --push upstream no_push \n</code></pre>    The last line prevents pushing to upstream. You can verify your remotes by <code>git remote -v</code> <pre><code>$&gt; git fetch upstream\n$&gt; git checkout master\n$&gt; git rebase upstream/master\n</code></pre></p> </li> <li> <p>Create a new branch to work on a feature or fix. Before this, please create an issue in the main Datashim repository that describes the problem or feature. Note the issue number (e.g. <code>nnn</code>) and assign it to yourself. In your local repository, create a branch to work on the fix. Use a short title (2 or 3 words) formed from the issue title/description along with the issue number as the branch name </p> </li> </ol> <p><pre><code>$&gt; git checkout -b nnn-short-title\n</code></pre>    Make your changes. Then commit your changes. Always sign your commits <pre><code>$&gt; git commit -s -m \"short descriptive message\"\n$&gt; git push $your_remote nnn-short-title\n</code></pre></p> <ol> <li> <p>When you are ready to submit a Pull Request (PR) for your completed feature or branch, visit your fork on Github and click the button titled <code>Compare and Pull Request</code> next to your <code>nnn-short-title</code> branch. This will submit the PR to Datashim.io for review</p> </li> <li> <p>After the review, prepare your PR for merging by squashing your commits. </p> </li> </ol>"},{"location":"GolangVSCodeGit/","title":"Recommended environment setup for development","text":""},{"location":"GolangVSCodeGit/#setting-up-go-and-vscode","title":"Setting up Go and VSCode","text":"<ol> <li> <p>Visit https://go.dev/doc/install to download and install Go on your computer. Alternatively, you can also use package managers for your operating system (e..g Homebrew for macOS)</p> </li> <li> <p>Once installed, run <code>go version</code> to verify that the installation is working</p> </li> <li> <p>(Recommended) Go uses a variable <code>GOPATH</code> to point to the current workspace. Package install commands such as <code>go install</code> will use this as their destination. If you are using a package as well as extending it, then it would be better to set up a separate workspace for development. To do this, create a separate directory, e.g. <code>$HOME/goprojects</code> and set it up with <code>bin</code>,<code>src</code>, and <code>pkg</code> sub-directories, and set <code>GOPATH</code> to point to it when developing. You can also use VSCode to modify <code>GOPATH</code> per project (see below)</p> </li> <li> <p>Download VSCode from https://code.visualstudio.com/download. Open Extensions tab and search for Go or go to https://marketplace.visualstudio.com/items?itemName=golang.go. Verify that the extension is by Go team at Google. Install extension to VSCode and test it with a sample program</p> </li> </ol>"},{"location":"GolangVSCodeGit/#setting-up-datashim-in-vscode","title":"Setting up Datashim in VSCode","text":"<p>Before following the below suggestions, please ensure that you have checked out Datashim following the git workflow for development. Datashim is a collection of multiple Go projects including the Dataset Operator, CSI-S3, Ceph Cache plugin, etc. Therefore, the VSCode setup is not as straightforward as with a single Go project. </p> <ol> <li> <p>Start VSCode. Open a new window (File -&gt; New Window). Select the Explorer view (generally the topmost icon on the left pane)</p> </li> <li> <p>Add a folder to the workspace (File -&gt; Add Folder To Workspace). In the file picker dialog, traverse to <code>$HOME/goprojects/src/github.com/$user/datashim</code> and then deeper into subprojects (i.e. <code>src/</code> folder). At this point, add the subfolder representing the project that you want to work on (e.g. <code>dataset-operator</code>). Do not add the project root folder to the VSCode workspace.</p> </li> <li> <p>Your Explorer view will have the project in the side panel like so:</p> </li> </ol> <p></p> <ol> <li>If you have followed the advice of having a separate directory for go projects, you need to inform Go plugin in VSCode about it. Open Preferences -&gt; Settings. Click on User or Workspace tab. On the left pane, click on Extensions -&gt; Go and scroll down to Gopath on the right-hand pane like so:</li> </ol> <p> </p> <ol> <li>Add these lines to the JSON file:    <pre><code>\"go.toolsGopath\": \"$HOME/go\",\n\"go.gopath\": \"$HOME/goprojects\",\n</code></pre>    where the first line is the Go installation folder and the second line is the folder you've created for hacking.</li> </ol>"},{"location":"Roadmap/","title":"Roadmap","text":"<p>The order of the features/milestones represents loosely the order of which development will start.</p>"},{"location":"Roadmap/#noobaa-caching-plugin","title":"Noobaa Caching Plugin","text":"<p>The S3-to-S3 caching is currently only supported by the Ceph/Rook-based plugin. However, we have been facing various problems as it's setup/configuration is not fully dynamic the way Noobaa is.</p> <p>In the wiki Caching-Remote-Buckets-(User-Guide) we have few hints about how to provision the cache buckets and this logic would be reflected on the Noobaa Caching Plugin</p>"},{"location":"Roadmap/#object-bucket-api","title":"Object Bucket API","text":"<p>Our current approach is based on our modified version of csi-s3 which is not maintained. The Object Bucket API will reduce the code we have to maintain as the S3 operations would be supported in a more K8s native manner with the new API.</p> <p>All the S3-related operations should be replaced with the Object Bucket API once it's ready to be used.</p>"},{"location":"Roadmap/#vault-based-access-management","title":"Vault-based access management","text":"<p>In our current approach, for the datasets which require credentials are stored in secrets. Secrets is the de-facto kubernetes solution for storing credentials. However there are some problems when it comes to datasets. We might want to restrict the access to the datasets between the users in the same namespace. We would be able to support scenarios where UserA and UserB are on the same namespace but UserA has datasets which only they can access.</p> <p>Plan to leverage TSI</p>"},{"location":"Roadmap/#spectrum-scale-caching-plugin","title":"Spectrum Scale Caching Plugin","text":"<p>Assuming Spectrum Scale installed on hosts we could leverage ibm-spectrum-scale-csi to provide the same functionality of S3 caching as Ceph-based and Noobaa-based.</p>"},{"location":"Roadmap/#dataset-eviction-from-cache","title":"Dataset Eviction from cache","text":"<p>In our current approach, in the one implementation we have of a caching plugin, every dataset is being cached without priorities or checks (whether the cache is full etc). We need to tackle this. </p> <p>The most naive way to solve it is to not to use cache for a newly created dataset when the cache is full. A more sophisticated approach would be to monitor the usage of datasets and decide to evict based on some configurable policies.</p>"},{"location":"Roadmap/#sequential-transformation-of-datasets","title":"Sequential Transformation of Datasets","text":"<p>In our current approach, the only possible transformation we have is Dataset -&gt; DatasetInternal -&gt; PVCs. In the future we would like to be able to support any number of transformation of any type. So there would be plugins that can handle a flow like this: Dataset(s3) -(caching)-&gt; DatasetInternal(s3) -(expose)-&gt; DatasetInternal(NFS) -&gt; PVC That would give the users the capability to cache and export their datasets in the format of their preference.</p>"},{"location":"Roadmap/#simple-scheduling-hints","title":"Simple Scheduling Hints","text":"<p>Since we are aware of the nodes where a dataset is cached we can potentially offer this information to external schedulers or decorate the pods using <code>nodeAffinity</code> to assist the default Kubernetes scheduler to place the pods closer to the cached data. This is expected to improve the performance of the pods using the specific datasets.</p>"},{"location":"cert-manager/","title":"Using cert-manager to rotate TLS certificates in Datashim","text":"<p>cert-manager is a X.509 certificate controller for Kubernetes and OpenShift workloads, capable of provisioning self-issued certificates, setting up an in-house CA, and integrating with publicly available CAs (e.g., letsencrypt).</p> <p>Info</p> <p>All the code samples below assume Datashim is installed in the <code>dlf</code> namespace.</p>"},{"location":"cert-manager/#installation","title":"Installation","text":"<p>In this document we will not go through how to deploy cert-manager and instead let the reader choose their preferred way to do so among the available ones listed on https://cert-manager.io/docs/installation/.</p>"},{"location":"cert-manager/#requesting-the-certificate","title":"Requesting the Certificate","text":"<p>To get started with cert-manager, we will have to first create a namespaced <code>Issuer</code> that will be able to issue us the certificate. We can simply <code>apply</code> the following YAML to create a self-signed <code>Issuer</code>:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Issuer\nmetadata:\n  name: datashim-issuer\n  namespace: dlf\nspec:\n  selfSigned: {}\n</code></pre> <p>With this <code>Issuer</code> we are now able to provision a <code>Certificate</code> for the webhook server by applying the following YAML:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: webhook-server-tls\n  namespace: dlf\nspec:\n  secretName: webhook-server-tls\n  secretTemplate:\n    labels:\n      app.kubernetes.io/name: dlf\n  duration: 8760h # 365d\n  renewBefore: 360h # 15d\n  isCA: false\n  privateKey:\n    algorithm: RSA\n    encoding: PKCS1\n    size: 2048\n  usages:\n    - server auth\n    - client auth\n  dnsNames:\n    - webhook-server.dlf.svc\n  issuerRef:\n    name: datashim-issuer\n    kind: Issuer\n    group: cert-manager.io\n</code></pre> <p>This <code>Certificate</code> object will cause cert-manager to rotate the certificate 15 days before its expiration and provision a certificate valid 365 days.</p> <p>To force cert-manager to provision the certificate we can manually delete the associated secret with:</p> <pre><code>kubectl delete secret -n dlf webhook-server-tls\n</code></pre>"},{"location":"cert-manager/#restarting-the-webhook-server","title":"Restarting the webhook server","text":"<p>After provisioning the new certificate, we need to ensure the webhook server picks up the new secret. We can do this by running:</p> <pre><code>kubectl delete pod -n dlf -l name=dataset-operator\n</code></pre>"},{"location":"cert-manager/#patching-the-mutatingwebhookconfiguration","title":"Patching the MutatingWebhookConfiguration","text":"<p>As the <code>MutatingWebhookConfiguration</code> contains the CA used to create the certificate, we need sync it with the one that cert-manager has used. Simply run:</p> <pre><code>CABUNDLE=$(kubectl get secret -n dlf webhook-server-tls -o jsonpath='{.data.ca\\.crt}')\nkubectl patch mutatingwebhookconfiguration -n dlf dlf-mutating-webhook-cfg --type='json' -p=\"[{'op': 'replace', 'path': '/webhooks/0/clientConfig/caBundle', 'value': \\\"$CABUNDLE\\\"}]\"\n</code></pre>"},{"location":"use-cases/","title":"Use cases","text":""},{"location":"use-cases/#simplify-generative-ai-model-development-on-kubernetes-with-datashim","title":"Simplify Generative AI Model Development on Kubernetes with\u00a0Datashim","text":"<p>[!NOTE] This tutorial is part of a Medium article that you can find on Medium</p> <p>(Note: Credit to YAML file from @zioproto for TGI deployment in Kubernetes which provided the basis for the TGI deployment shown in this example)</p>"},{"location":"use-cases/#prerequisites","title":"Prerequisites","text":"<p>Please read the Medium article we have written on Medium understand the context of this tutorial.</p> <p>Other than that, there are no prerequisites needed to follow this tutorial, as it will provide instructions to provision a local S3 endpoint and store two models in it. If you already have them, feel free to skip the optional instructions, but make sure to update the values in the YAMLs, as they will all reference the setup we provide.</p>"},{"location":"use-cases/#optional-creating-a-local-object-storage-endpoint","title":"(OPTIONAL) Creating a local object storage endpoint","text":"<p>The YAML we provide provisions a local MinIO instance using hardcoded credentials.</p> <p>[!CAUTION]  Do not use this for any real production workloads!</p> <p>From this folder, simply run:</p> <pre><code>kubectl create namespace minio\nkubectl apply -f minio.yaml\nkubectl wait pod --for=condition=Ready -n minio --timeout=-1s minio\n</code></pre>"},{"location":"use-cases/#creating-the-staging-and-production-namespaces","title":"Creating the staging and production namespaces","text":"<p>Let us start by creating the staging and production namespaces:</p> <pre><code>kubectl create namespace production\nkubectl create namespace staging\n</code></pre> <p>To use Datashim's functionalities, we must also label them with <code>monitor-pods-datasets=enabled</code> so that Datashim can mount volumes in the pods:</p> <pre><code>kubectl label namespace production monitor-pods-datasets=enabled\nkubectl label namespace staging monitor-pods-datasets=enabled\n</code></pre>"},{"location":"use-cases/#creating-the-datasets","title":"Creating the Datasets","text":"<p>To access our data, we must first create a <code>Secret</code> containing the credentials to access the bucket that holds our data, and then a <code>Dataset</code> object that links configuration information to the access credentials.</p> <p>Run</p> <pre><code>kubectl apply -f s3-secret-prod.yaml\nkubectl apply -f dataset-prod.yaml\nkubectl apply -f s3-secret-staging.yaml\nkubectl apply -f dataset-staging.yaml\n</code></pre> <p>To create secrets holding the access information to our local S3 endpoint and the related Datasets you can see in the \"A use case: model development on Kubernetes\" section of the article.</p>"},{"location":"use-cases/#optional-adding-models-in-the-object-storage","title":"(OPTIONAL) Adding models in the object storage","text":"<p>In this tutorial we simulate a development team working with two different models: FLAN-T5-Small will be our \"production\" model, while the bigger and improved FLAN-T5-Base will be our \"staging\" model. To load them in our MinIO instance we can run:</p> <pre><code>kubectl apply -f download-flan-t5-small-to-minio-prod.yaml\nkubectl wait -n production --for=condition=complete job/download-flan --timeout=-1s\nkubectl apply -f download-flan-t5-base-to-minio-staging.yaml\nkubectl wait -n staging --for=condition=complete job/download-flan --timeout=-1s\n</code></pre> <p>This will create two Jobs that will download the appropriate model for each namespace, and wait for their completion. This may take several minutes.</p> <p>[!NOTE] Using git to clone directly in <code>/mnt/datasets/model-weights/my-model/</code> would fail on OpenShift due to the default security policies. Errors such as <code>cp: can't preserve permissions</code> you might see in the pod logs can be safely ignored.</p>"},{"location":"use-cases/#creating-the-tgi-deployments","title":"Creating the TGI deployments","text":"<p>As we mention in the article, we can now use the same Deployment file to serve the model in both namespaces. Run:</p> <pre><code>kubectl apply -n production -f deployment.yaml\nkubectl apply -n staging -f deployment.yaml\n</code></pre> <p>To create the TGI deployments. We can wait for TGI to be ready using the command:</p> <pre><code>kubectl wait pod -n production -l run=text-generation-inference --for=condition=Ready --timeout=-1s\nkubectl wait pod -n staging -l run=text-generation-inference --for=condition=Ready --timeout=-1s\n</code></pre>"},{"location":"use-cases/#creating-the-tgi-service","title":"Creating the TGI service","text":"<p>We can now create a service in both namespaces as such:</p> <pre><code>kubectl apply -n production -f service.yaml\nkubectl apply -n staging -f service.yaml\n</code></pre>"},{"location":"use-cases/#validating-the-deployment","title":"Validating the deployment","text":"<p>We can now forward the service exposing TGI as such:</p> <pre><code>kubectl port-forward -n production --address localhost svc/text-generation-inference 8888:8080 &amp;\nkubectl port-forward -n staging --address localhost svc/text-generation-inference 8889:8080 &amp;\n</code></pre> <p>And run an inference request against it with:</p> <pre><code>curl -s http://localhost:8888/generate -X POST -d '{\"inputs\":\"The square root of x is the cube root of y. What is y to the power of 2, if x = 4?\", \"parameters\":{\"max_new_tokens\":1000}}'  -H 'Content-Type: application/json' | jq -r .generated_text\ncurl -s http://localhost:8889/generate -X POST -d '{\"inputs\":\"The square root of x is the cube root of y. What is y to the power of 2, if x = 4?\", \"parameters\":{\"max_new_tokens\":1000}}'  -H 'Content-Type: application/json' | jq -r .generated_text\n</code></pre> <p>The flan-t5-small should be very fast and reply with:</p> <pre><code>0\n</code></pre> <p>flan-t5-base will instead take a while to reply with:</p> <pre><code>x = 4 * 2 = 8 x = 16 y = 16 to the power of 2\n</code></pre>"},{"location":"use-cases/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Get started with Datashim</p> <p>Check out our User Guide and get up and running in minutes</p> <p> User Guide</p> </li> <li> <p> Any questions?</p> <p>Find answers to frequently asked questions in our FAQ</p> <p> FAQ</p> </li> </ul>"},{"location":"user-guide/","title":"User guide","text":""},{"location":"user-guide/#using-datasets-with-secret-references","title":"Using Datasets with Secret references","text":"<p>While Datashim supports including bucket credentials in the Dataset definition, this is insecure and should be avoided. We recommend storing credentials in a Kubernetes Secret object, which can then be referenced in the Dataset definition.</p> <p>Given the following Secret definition:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-dataset-secret\nstringData:\n  accessKeyID: \"ACCESS_KEY\"\n  secretAccessKey: \"SECRET_KEY\"\n</code></pre> <p>We can create a Dataset without hardcoded credentials as such:</p> <pre><code>apiVersion: datashim.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: my-dataset\nspec:\n  local:\n    bucket: my-bucket\n    endpoint: http://my-s3-endpoint\n    secret-name: my-dataset-secret\n    type: COS\n</code></pre>"},{"location":"user-guide/#provisioning-buckets-via-the-dataset","title":"Provisioning buckets via the Dataset","text":"<p>When you define a Dataset, you can either use an existing bucket or ask Datashim to create the one referenced in the Dataset automatically. This can be done by including <code>provision: \"true\"</code> in the Dataset definition as shown below:</p> <pre><code>apiVersion: datashim.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: my-dataset\nspec:\n  local:\n    provision: \"true\" # &lt;----\n    bucket: my-bucket\n    endpoint: http://my-s3-endpoint\n    secret-name: my-dataset-secret\n    type: COS\n</code></pre>"},{"location":"user-guide/#creating-read-only-datasets","title":"Creating read-only Datasets","text":"<p>There are circumnstances where we want people to be able to access the contents of a bucket but not be able to modify them. While this can (and should) be done by creating a set of credentials with only \"reader\" permissions on the bucket, Datashim supports creating read-only Datasets by specifying the <code>readonly: \"true\"</code> option, as such:</p> <pre><code>apiVersion: datashim.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: my-dataset\nspec:\n  local:\n    readonly: \"true\" # &lt;----\n    bucket: my-bucket\n    endpoint: http://my-s3-endpoint\n    secret-name: my-dataset-secret\n    type: COS\n</code></pre>"},{"location":"user-guide/#creating-datasets-on-bucket-subpaths","title":"Creating Datasets on bucket subpaths","text":"<p>In most cases, S3 credentials give users access to all buckets in an instance and all their subpaths. When it comes to datasets, however, we might be interested in limiting access to a particular \"folder\", or sub-path. When creating a Dataset, we can specify the <code>folder</code> option to limit access, as shown below:</p> <pre><code>apiVersion: datashim.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: my-dataset\nspec:\n  local:\n    bucket: my-bucket/my-user/data # &lt;----\n    endpoint: http://my-s3-endpoint\n    secret-name: my-dataset-secret\n    type: COS\n</code></pre>"},{"location":"user-guide/#deleting-a-bucket-on-dataset-deletion","title":"Deleting a bucket on Dataset deletion","text":"<p>We might want to tie the lifecycle of a bucket to that of a Dataset by creating it and deleting it along with the Dataset. In addition to the <code>provision</code> option mentioned earlier, Datashim allows deleting a bucket when a Dataset is deleted with the <code>removeOnDelete</code> option.</p> <pre><code>apiVersion: datashim.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: my-dataset\nspec:\n  local:\n    provision: \"true\"\n    removeOnDelete: \"true\" # &lt;----\n    bucket: my-bucket\n    endpoint: http://my-s3-endpoint\n    secret-name: my-dataset-secret\n    type: COS\n</code></pre>"},{"location":"user-guide/#creating-datasets-from-archives","title":"Creating Datasets from archives","text":"<p>Warning</p> <p>For using archive Datasets, a secret called <code>minio-conf</code> must be present in the namespace where Datashim is installed, typically <code>dlf</code>.</p> <p>To deploy a MinIO instance in the <code>dlf</code> namespace (and automatically create the <code>minio</code> secret) you can use the following oneliner: <pre><code>  kubectl apply -n dlf -f https://github.com/datashim-io/datashim/raw/master/examples/minio/minio.yaml\n</code></pre></p> <p>NOTE: use this only as a reference point. For production, make sure sure  appropriate and secure credentials are used</p> <p>Datashim allows creating Datasets from archive files with the <code>ARCHIVE</code> dataset type. The archive will be downloaded and uploaded to the S3 backing store described by the <code>minio-conf</code> Secret. An additional option for extracting the </p> <p>An example Dataset of the archive type is provided:</p> <pre><code>apiVersion: datashim.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: archive-dataset\nspec:\n  type: \"ARCHIVE\"\n  url: \"https://dax-cdn.cdn.appdomain.cloud/dax-noaa-weather-data-jfk-airport/1.1.4/noaa-weather-data-jfk-airport.tar.gz\"\n  format: \"application/x-tar\"\n  extract: \"true\" # &lt;---- OPTIONAL, to extract the content of the archive\n</code></pre>"},{"location":"user-guide/#next-steps","title":"Next steps","text":"<ul> <li> <p> Even more!</p> <p>You can read up about Datashim's more advanced features in our Advanced Usage section</p> <p> Advanced Usage</p> </li> <li> <p> Any questions?</p> <p>Find answers to frequently asked questions in our FAQ</p> <p> FAQ</p> </li> </ul>"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/","title":"Data Volumes for Notebook Servers","text":"<p>We will show how you can use DLF to provision Data Volumes for your notebook servers. This would be helpful in the cases your training data are stored in S3 Buckets.</p>"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/#requirements","title":"Requirements","text":"<p>You have access to the kubeflow dashboard and you have DLF installed.</p> <p>Make sure you first follow the guide for Installation</p>"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/#create-a-dataset-for-the-s3-bucket","title":"Create a Dataset for the S3 Bucket","text":"<p>In this guide, we assume that your data are already stored in a remote s3 bucket. Let's assume that you will launch your notebook server on the namespace <code>{my-namespace}</code></p> <p><pre><code>apiVersion: datashim.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: your-dataset\nspec:\n  local:\n    type: \"COS\"\n    accessKeyID: \"access_key_id\"\n    secretAccessKey: \"secret_access_key\"\n    endpoint: \"https://YOUR_ENDPOINT\"\n    bucket: \"YOUR_BUCKET\"\n    region: \"\" #it can be empty\n</code></pre> Now just execute: <pre><code>kubectl create -f my-dataset.yaml -n {my-namespace}\n</code></pre></p>"},{"location":"kubeflow/Data-Volumes-for-Notebook-Servers/#provision-notebook-with-the-data-volume","title":"Provision Notebook with the Data Volume","text":"<p>Now use the Kubeflow Central Dashboard to follow the rest of the guide. Choose the \"Notebook Servers\" item:</p> <p></p> <p>Select \"New server\":</p> <p></p> <p>Head over to the \"Data Volumes\" section and fill out the form as follows:</p> <p></p> <p>Now you can press \"Launch\" to start the notebook server.</p> <p>After you connect, you can list the contents of <code>/mnt/dataset</code> and verity that the reflect the contents for your remote S3 bucket. NOTE: all the changes that you do in this directory (delete,create,modify) will be reflected on the remote bucket</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/","title":"Model Storing and Serving","text":"<p>We will show how you can use DLF to store/serve trained models using S3 Buckets.</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#requirements","title":"Requirements","text":"<p>You have permissions in a namespace where you can use for kubeflow (to create TFJobs, deployments etc) Lets assume the namespace you can use is <code>{my-namespace}</code>. Feel free to change accordingly.</p> <p>Make sure you first follow the guide for Installation</p> <p>We will loosely follow the example posted in mnist_vanilla_k8s.ipynb</p> <p>NOTE: All example yaml files mentioned in the wiki are also available in examples/kubeflow</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#build-model-container","title":"Build model container","text":"<p>There is a delta between existing distributed mnist examples and what's needed to run well as a TFJob. We will skip the kaniko part and just build and use the Dockerfile and model.py in examples/kubeflow</p> <pre><code>cd examples/kubeflow\ndocker build -t {MY-REGISTRY}/mnist-model -f Dockerfile.model .\ndocker push {MY-REGISTRY}/mnist-model\n</code></pre> <p>In case you use an authenticated registry, follow the instructions in configure-docker-credentials</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#create-an-s3-bucket-and-its-dataset","title":"Create an S3 Bucket and its Dataset","text":"<p>If you have an existing s3 bucket you can use, please proceed with this one. Otherwise follow the instructions in Configure IBM COS Storage</p> <p>Now we need to create a dataset to point to the newly created bucket. Create a file that looks like this: <pre><code>apiVersion: datashim.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: your-dataset\nspec:\n  local:\n    type: \"COS\"\n    accessKeyID: \"access_key_id\"\n    secretAccessKey: \"secret_access_key\"\n    endpoint: \"https://YOUR_ENDPOINT\"\n    bucket: \"YOUR_BUCKET\"\n    region: \"\" #it can be empty\n</code></pre> Now just execute: <pre><code>kubectl create -f my-dataset.yaml -n {my-namespace}\n</code></pre></p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#launch-a-tfjob","title":"Launch a TFJob","text":"<p>Now we are ready to launch a tfjob in a much less verbose way since DLF takes care of mounting the dataset and providing access to the tensorflow pod: <pre><code>apiVersion: kubeflow.org/v1\nkind: TFJob\nmetadata:\n  name: my-train\nspec:\n  tfReplicaSpecs:\n    Ps:\n      replicas: 1\n      template:\n        metadata:\n          labels:\n           dataset.0.id: \"your-dataset\"\n           dataset.0.useas: \"mount\"\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n        spec:\n          serviceAccount: default-editor\n          containers:\n            - name: tensorflow\n              command:\n                - python\n                - /opt/model.py\n                - --tf-model-dir=/mnt/datasets/your-dataset/mnist\n                - --tf-export-dir=/mnt/datasets/your-dataset/mnist/export\n                - --tf-train-steps=200\n                - --tf-batch-size=100\n                - --tf-learning-rate=0.1\n              image: yiannisgkoufas/mnist\n              workingDir: /opt\n              resources:\n                limits:\n                  ephemeral-storage: \"10Gi\"\n                requests:\n                  ephemeral-storage: \"10Gi\"\n          restartPolicy: OnFailure\n    Chief:\n      replicas: 1\n      template:\n        metadata:\n          labels:\n            dataset.0.id: \"your-dataset\"\n            dataset.0.useas: \"mount\"\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n        spec:\n          serviceAccount: default-editor\n          containers:\n            - name: tensorflow\n              resources:\n                limits:\n                  ephemeral-storage: \"10Gi\"\n                requests:\n                  ephemeral-storage: \"10Gi\"\n              command:\n                - python\n                - /opt/model.py\n                - --tf-model-dir=/mnt/datasets/your-dataset/mnist\n                - --tf-export-dir=/mnt/datasets/your-dataset/mnist/export\n                - --tf-train-steps=200\n                - --tf-batch-size=100\n                - --tf-learning-rate=0.1\n              image: yiannisgkoufas/mnist\n          restartPolicy: OnFailure\n    Worker:\n      replicas: 1\n      template:\n        metadata:\n          labels:\n            dataset.0.id: \"your-dataset\"\n            dataset.0.useas: \"mount\"\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n        spec:\n          serviceAccount: default-editor\n          containers:\n            - name: tensorflow\n              command:\n                - python\n                - /opt/model.py\n                - --tf-model-dir=/mnt/datasets/your-dataset/mnist\n                - --tf-export-dir=/mnt/datasets/your-dataset/mnist/export\n                - --tf-train-steps=200\n                - --tf-batch-size=100\n                - --tf-learning-rate=0.1\n              image: yiannisgkoufas/mnist\n              workingDir: /opt\n          restartPolicy: OnFailure\n</code></pre> Make sure to replace <code>your-dataset</code> with the name of your dataset. Create the TFJob like that: <pre><code>kubectl create -f tfjob.yaml -n {my-namespace}\n</code></pre> You should see the job running and the model stored in the end in the remote S3 bucket.</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#view-the-model-in-tensorboard","title":"View the Model in Tensorboard","text":"<p>You can inspect the model you created and stored in the remote S3 bucket by creating the following yaml file which again leverages the Dataset created. <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mnist-tensorboard\n  name: mnist-tensorboard\nspec:\n  selector:\n    matchLabels:\n      app: mnist-tensorboard\n  template:\n    metadata:\n      labels:\n        app: mnist-tensorboard\n        version: v1\n        dataset.0.id: \"your-dataset\"\n        dataset.0.useas: \"mount\"\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      serviceAccount: default-editor\n      containers:\n        - command:\n            - /usr/local/bin/tensorboard\n            - --logdir=/mnt/datasets/your-dataset/mnist\n            - --port=80\n          image: tensorflow/tensorflow:1.15.2-py3\n          name: tensorboard\n          ports:\n            - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: mnist-tensorboard\n  name: mnist-tensorboard\nspec:\n  ports:\n    - name: http-tb\n      port: 80\n      targetPort: 80\n  selector:\n    app: mnist-tensorboard\n  type: ClusterIP\n---\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: mnist-tensorboard\nspec:\n  gateways:\n    - kubeflow/kubeflow-gateway\n  hosts:\n    - '*'\n  http:\n    - match:\n        - uri:\n            prefix: /mnist/default/tensorboard/\n      rewrite:\n        uri: /\n      route:\n        - destination:\n            host: mnist-tensorboard.default.svc.cluster.local\n            port:\n              number: 80\n      timeout: 300s\n</code></pre> Create the deployment: <pre><code>kubectl create -f tensorboard.yaml -n {my-namespace}\n</code></pre> You can expose the service and access it remotely as described here: Tensorboard access</p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#model-serving-using-kfserving","title":"Model Serving Using KFServing","text":"<p>You can leverage DLF to run the inference service on the model you trained using KFServing as follows: <pre><code>apiVersion: \"serving.kubeflow.org/v1alpha2\"\nkind: \"InferenceService\"\nmetadata:\n  name: \"mnist-sample\"\nspec:\n  default:\n    predictor:\n      tensorflow:\n        storageUri: \"pvc://your-dataset/mnist/export\"\n</code></pre> Create the yaml: <pre><code>kubectl create -f kfserving-inference.yaml -n {my-namespace}\n</code></pre></p>"},{"location":"kubeflow/Model-Storing-and-Serving-with-DLF/#model-serving-using-tensorflow-serving","title":"Model Serving Using Tensorflow Serving","text":"<p>Again you can leverage DLF to serve the model you trained.  <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mnist\n  name: tensorflow-serving\nspec:\n  selector:\n    matchLabels:\n      app: mnist-model\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n      labels:\n        app: mnist-model\n        version: v1\n        dataset.0.id: \"your-dataset\"\n        dataset.0.useas: \"mount\"\n    spec:\n      serviceAccount: default-editor\n      containers:\n        - args:\n            - --port=9000\n            - --rest_api_port=8500\n            - --model_name=mnist\n            - --model_base_path=/mnt/datasets/your-dataset/mnist/export\n          command:\n            - /usr/bin/tensorflow_model_server\n          env:\n            - name: modelBasePath\n              value: /mnt/datasets/your-dataset/mnist/export\n          image: tensorflow/serving:1.15.0\n          imagePullPolicy: IfNotPresent\n          livenessProbe:\n            initialDelaySeconds: 30\n            periodSeconds: 30\n            tcpSocket:\n              port: 9000\n          name: mnist\n          ports:\n            - containerPort: 9000\n            - containerPort: 8500\n          resources:\n            limits:\n              cpu: \"4\"\n              memory: 4Gi\n            requests:\n              cpu: \"1\"\n              memory: 1Gi\n          volumeMounts:\n            - mountPath: /var/config/\n              name: model-config\n      volumes:\n        - configMap:\n            name: tensorflow-serving\n          name: model-config\n---\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/path: /monitoring/prometheus/metrics\n    prometheus.io/port: \"8500\"\n    prometheus.io/scrape: \"true\"\n  labels:\n    app: mnist-model\n  name: tensorflow-serving\nspec:\n  ports:\n    - name: grpc-tf-serving\n      port: 9000\n      targetPort: 9000\n    - name: http-tf-serving\n      port: 8500\n      targetPort: 8500\n  selector:\n    app: mnist-model\n  type: ClusterIP\n---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: tensorflow-serving\ndata:\n  monitoring_config.txt: |-\n    prometheus_config: {{\n      enable: true,\n      path: \"/monitoring/prometheus/metrics\"\n    }}\n</code></pre> Now create the deployment: <pre><code>kubectl create -f tensorflow-serving -n {my-namespace}\n</code></pre></p> <p>If you want to deploy the demo with the MNIST UI follow the instructions in MNIST UI</p>"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/","title":"PVCs for Pipelines SDK","text":"<p>We will show how you can use DLF to provision Persistent Volume Claims via DLF so you can use it within Pipelines SDK.</p>"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/#requirements","title":"Requirements","text":"<p>You have kubeflow installed and you can deploy pipelines using the Pipeline SDK.</p> <p>Make sure you first follow the guide for Installation</p> <p>We will just how you can adopt the examples located in contrib/volume_ops</p> <p>NOTE: For this guide you can use both an empty and pre-populated with data bucket.</p>"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/#example-with-creation-of-dataset-before-the-pipeline-execution","title":"Example with creation of Dataset before the pipeline execution","text":"<p>First you need to create a Dataset to point to the bucket you want to use. Create a file that looks like this: <pre><code>apiVersion: datashim.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: your-dataset\nspec:\n  local:\n    type: \"COS\"\n    accessKeyID: \"access_key_id\"\n    secretAccessKey: \"secret_access_key\"\n    endpoint: \"https://YOUR_ENDPOINT\"\n    bucket: \"YOUR_BUCKET\"\n    region: \"\" #it can be empty\n</code></pre> Now just execute: <pre><code>kubectl create -f my-dataset.yaml -n {my-namespace}\n</code></pre></p> <p>Now within <code>{my-namespace}</code> you will find a PVC which you can use within your pipelines SDK without a problem.</p> <p>You can see the example below which can use the PVC which was created out of your dataset. <pre><code>import kfp\nimport kfp.dsl as dsl\nfrom kfp.dsl import PipelineVolume\n\n\n@dsl.pipeline(\n    name=\"Volume Op DAG\",\n    description=\"The second example of the design doc.\"\n)\ndef volume_op_dag():\n\n    dataset = PipelineVolume(\"your-dataset\")\n\n    step1 = dsl.ContainerOp(\n        name=\"step1\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo 1|tee /data/file1\"],\n        pvolumes={\"/data\": dataset}\n    )\n\n    step2 = dsl.ContainerOp(\n        name=\"step2\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"cp /data/file1 /data/file2\"],\n        pvolumes={\"/data\": step1.pvolume}\n    )\n\n    step3 = dsl.ContainerOp(\n        name=\"step3\",\n        image=\"library/bash:4.4.23\",\n        command=[\"cat\", \"/mnt/file1\", \"/mnt/file2\"],\n        pvolumes={\"/mnt\": step2.pvolume}\n    )\n\n\n\nif __name__ == \"__main__\":\n    import kfp.compiler as compiler\n    compiler.Compiler().compile(volume_op_dag, __file__ + \".tar.gz\")\n</code></pre></p>"},{"location":"kubeflow/PVCs-for-Pipelines-SDK/#example-with-creation-of-dataset-as-part-of-the-pipeline-execution","title":"Example with creation of Dataset as part of the pipeline execution","text":"<p>If instead you want to create a Dataset as part of your pipeline, you can create the Dataset yaml and invoke a <code>ResourceOp</code>.</p> <p>Before that you need to make sure that the service account <code>pipeline-runner</code> in namespace <code>kubeflow</code> can create/delete Datasets, so make sure you execute <code>kubectl apply -f examples/kubeflow/pipeline-runner-binding.yaml</code> before running the pipeline. The example rolebinding definition is in examples/kubeflow/pipeline-runner-binding.yaml</p> <p>In the following pipeline we are creating the Dataset in step0 and then proceed to step1 to use it:</p> <pre><code>import kfp.dsl as dsl\nimport yaml\nfrom kfp.dsl import PipelineVolume\n\n# Make sure that you have applied ./pipeline-runner-binding.yaml\n# or any serviceAccount that should be allowed to create/delete datasets\n\n@dsl.pipeline(\n    name=\"Volume Op DAG\",\n    description=\"The second example of the design doc.\"\n)\ndef volume_op_dag():\n\n    datasetName = \"your-dataset\"\n    dataset = PipelineVolume(datasetName)\n\n    step0 = dsl.ResourceOp(name=\"dataset-creation\",k8s_resource=get_dataset_yaml(\n        datasetName,\n        \"XXXXXXXXXXXXXXX\",\n        \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n        \"http://your_endpoint.com\",\n        \"bucket-name\",\n        \"\"\n    ))\n\n    step1 = dsl.ContainerOp(\n        name=\"step1\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo 1|tee /data/file1\"],\n        pvolumes={\"/data\": dataset}\n    ).after(step0)\n\n    step2 = dsl.ContainerOp(\n        name=\"step2\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"cp /data/file1 /data/file2\"],\n        pvolumes={\"/data\": step1.pvolume}\n    )\n\n    step3 = dsl.ContainerOp(\n        name=\"step3\",\n        image=\"library/bash:4.4.23\",\n        command=[\"cat\", \"/mnt/file1\", \"/mnt/file2\"],\n        pvolumes={\"/mnt\": step2.pvolume}\n    )\n\ndef get_dataset_yaml(name,accessKey,secretAccessKey,endpoint,bucket,region):\n    print(region)\n    dataset_spec = f\"\"\"\n    apiVersion: datashim.io/v1alpha1\n    kind: Dataset\n    metadata:\n      name: {name}\n    spec:\n      local:\n        type: \"COS\"\n        accessKeyID: {accessKey}\n        secretAccessKey: {secretAccessKey}\n        endpoint: {endpoint}\n        bucket: {bucket}\n        region: {region}\n    \"\"\"\n    data = yaml.safe_load(dataset_spec)\n    convert_none_to_str(data)\n    return data\n</code></pre>"}]}